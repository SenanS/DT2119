Index: Lab 2/lab2_proto.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nimport lab2_tools\nfrom prondict import prondict\nimport matplotlib.pyplot as plt\n\n\ndef concatTwoHMMs(hmm1, hmm2):\n    \"\"\" Concatenates 2 HMM models\n\n    Args:\n       hmm1, hmm2: two dictionaries with the following keys:\n           name: phonetic or word symbol corresponding to the model\n           startprob: M+1 array with priori probability of state\n           transmat: (M+1)x(M+1) transition matrix\n           means: MxD array of mean vectors\n           covars: MxD array of variances\n\n    D is the dimension of the feature vectors\n    M is the number of emitting states in each HMM model (could be different for each)\n\n    Output\n       dictionary with the same keys as the input but concatenated models:\n          startprob: K+1 array with priori probability of state\n          transmat: (K+1)x(K+1) transition matrix\n             means: KxD array of mean vectors\n            covars: KxD array of variances\n\n    K is the sum of the number of emitting states from the input models\n   \n    Example:\n       twoHMMs = concatHMMs(phoneHMMs['sil'], phoneHMMs['ow'])\n\n    See also: the concatenating_hmms.pdf document in the lab package\n    \"\"\"\n    size1 = np.size(hmm1['startprob']) - 1\n    size2 = np.size(hmm2['startprob']) - 1\n\n    concat_hmm = {}\n    concat_hmm['startprob'] = hmm2['startprob'] * hmm1['startprob'][size1]\n    concat_hmm['startprob'] = np.concatenate((hmm1['startprob'][0:size1], concat_hmm['startprob']))\n\n    mul = np.reshape(hmm1['transmat'][0:-1, -1], (size1, 1)) @ np.reshape(hmm2['startprob'], (1, size2+1))\n    concat_hmm['transmat'] =  np.concatenate((hmm1['transmat'][0:-1, 0:-1], mul), axis=1)\n\n    tmp = np.concatenate((np.zeros([size2+1, size1]), hmm2['transmat']), axis=1)\n    concat_hmm['transmat'] = np.concatenate((concat_hmm['transmat'], tmp), axis=0)\n\n    concat_hmm['means'] = np.concatenate((hmm1['means'], hmm2['means']), axis=0)\n    concat_hmm['covars'] = np.concatenate((hmm1['covars'], hmm2['covars']), axis=0)\n\n    return concat_hmm\n\n\n# this is already implemented, but based on concat2HMMs() above\ndef concatHMMs(hmmmodels, namelist):\n    \"\"\" Concatenates HMM models in a left to right manner\n\n    Args:\n       hmmmodels: dictionary of models indexed by model name. \n       hmmmodels[name] is a dictionaries with the following keys:\n           name: phonetic or word symbol corresponding to the model\n           startprob: M+1 array with priori probability of state\n           transmat: (M+1)x(M+1) transition matrix\n           means: MxD array of mean vectors\n           covars: MxD array of variances\n       namelist: list of model names that we want to concatenate\n\n    D is the dimension of the feature vectors\n    M is the number of emitting states in each HMM model (could be\n      different in each model)\n\n    Output\n       combinedhmm: dictionary with the same keys as the input but\n                    combined models:\n         startprob: K+1 array with priori probability of state\n          transmat: (K+1)x(K+1) transition matrix\n             means: KxD array of mean vectors\n            covars: KxD array of variances\n\n    K is the sum of the number of emitting states from the input models\n\n    Example:\n       wordHMMs['o'] = concatHMMs(phoneHMMs, ['sil', 'ow', 'sil'])\n    \"\"\"\n    concat = hmmmodels[namelist[0]]\n    for idx in range(1, len(namelist)):\n        concat = concatTwoHMMs(concat, hmmmodels[namelist[idx]])\n    return concat\n\n\n\ndef gmmloglik(log_emlik, weights):\n    \"\"\"Log Likelihood for a GMM model based on Multivariate Normal Distribution.\n\n    Args:\n        log_emlik: array like, shape (N, K).\n            contains the log likelihoods for each of N observations and\n            each of K distributions\n        weights:   weight vector for the K components in the mixture\n\n    Output:\n        gmmloglik: scalar, log likelihood of data given the GMM model.\n    \"\"\"\n    # # GMMLOGLIK still in testing\n    # loglik_gmm = 0\n    # loglik_gmm += lab2_tools.logsumexp(log_emlik[:, :] + np.log(weights))\n    #\n    # gmm = 0\n    # for i in range(log_emlik.shape[0]):\n    #     gmm += lab2_tools.logsumexp(log_emlik[i, :] + np.log(weights))\n    #\n    # print(gmm - loglik_gmm)\n    #\n    #\n    # return loglik_gmm\n\n\n\ndef forward(log_emlik, log_startprob, log_transmat):\n    \"\"\"Forward (alpha) probabilities in log domain.\n\n    Args:\n        log_emlik: NxM array of emission log likelihoods, N frames, M states\n        log_startprob: log probability to start in state i\n        log_transmat: log transition probability from state i to j\n\n    Output:\n        forward_prob: NxM array of forward log probabilities for each of the M states in the model\n    \"\"\"\n\n    N, M = log_emlik.shape\n\n    # Create alpha return matrix, populate with n=0 formula result.\n    forward_prob = np.zeros((N, M))\n\n    forward_prob[0, :] = log_startprob[:-1] + log_emlik[0, :]\n\n    for n in range(1, N):\n        for j in range(M):\n            forward_prob[n, j] = lab2_tools.logsumexp(forward_prob[n-1, :] + log_transmat[:-1, j]) + log_emlik[n, j]\n\n    return forward_prob\n\n\ndef backward(log_emlik, log_startprob, log_transmat):\n    \"\"\"Backward (beta) probabilities in log domain.\n\n    Args:\n        log_emlik: NxM array of emission log likelihoods, N frames, M states\n        log_startprob: log probability to start in state i\n        log_transmat: transition log probability from state i to j\n\n    Output:\n        backward_prob: NxM array of backward log probabilities for each of the M states in the model\n    \"\"\"\n\n    N, M = log_emlik.shape\n\n    # Create zeroed beta return matrix.\n    log_beta = np.zeros((N, M))\n\n    #For all other n, populate beta with regular formula result.\n    #Start at N-2 &, in increments of -1, finish at 0.\n    for n in range(N - 2, -1, -1):\n        for j in range(M):\n            log_beta[n][j] = lab2_tools.logsumexp(log_beta[n + 1, :] + log_emlik[n + 1, :] + log_transmat[j, :-1])\n\n    return log_beta\n\n\ndef viterbi(log_emlik, log_startprob, log_transmat, forceFinalState=True):\n    \"\"\"Viterbi path.\n\n    Args:\n        log_emlik: NxM array of emission log likelihoods, N frames, M states\n        log_startprob: log probability to start in state i\n        log_transmat: transition log probability from state i to j\n        forceFinalState: if True, start backtracking from the final state in\n                  the model, instead of the best state at the last time step\n\n    Output:\n        viterbi_loglik: log likelihood of the best path\n        viterbi_path: best path\n    \"\"\"\n\n    N, M = log_emlik.shape\n\n    log_viterbi = np.zeros((N, M))\n    viterbi_path = np.zeros(N)\n    backtrack_matrix = np.zeros((N, M))\n\n    # Populate viterbi matrix with with n=0 formula result.\n    log_viterbi[0][:] = np.add(log_startprob[:-1], log_emlik[0][:])\n\n    # For all other n, populate viterbi with regular recursive formula result.\n    for n in range(1, N):\n        for j in range(M):\n            # Store the highest likelihood and it's index.\n            B_n = log_viterbi[n - 1, :] + log_transmat[:-1, j]\n            log_viterbi[n, j] = log_emlik[n, j] + np.max(B_n)\n            backtrack_matrix[n, j] = np.argmax(B_n)\n\n    # Setup path variable, depending on forceFinalState.\n    if forceFinalState:\n        viterbi_path[N - 1] = M - 1\n    else:\n        viterbi_path[N - 1] = np.argmax(log_viterbi[N - 1, :])\n    # Go through each column of the matrix backwards to find the route of the highest likelihood.\n    for i in range(N - 2, -1, -1):\n        viterbi_path[i] = backtrack_matrix[i + 1, int(viterbi_path[i + 1])]\n\n    #Get best score\n    viterbi_loglik = np.max(log_viterbi[-1, :])\n\n    return viterbi_loglik, viterbi_path\n\n\ndef statePosteriors(log_alpha, log_beta):\n    \"\"\"State posterior (gamma) probabilities in log domain.\n\n    Args:\n        log_alpha: NxM array of log forward (alpha) probabilities\n        log_beta: NxM array of log backward (beta) probabilities\n    where N is the number of frames, and M the number of states\n\n    Output:\n        log_gamma: NxM array of gamma probabilities for each of the M states in the model\n    \"\"\"\n    log_gamma = log_alpha + log_beta - lab2_tools.logsumexp(log_alpha[-1,:])\n    return log_gamma\n\ndef updateMeanAndVar(X, log_gamma, varianceFloor=5.0):\n    \"\"\" Update Gaussian parameters with diagonal covariance\n\n    Args:\n         X: NxD array of feature vectors\n         log_gamma: NxM state posterior probabilities in log domain\n         varianceFloor: minimum allowed variance scalar\n    were N is the lenght of the observation sequence, D is the\n    dimensionality of the feature vectors and M is the number of\n    states in the model\n\n    Outputs:\n         means: MxD mean vectors for each state\n         covars: MxD covariance (variance) vectors for each state\n    \"\"\"\n\n    N, D = X.shape\n    M = log_gamma.shape[1]\n\n    means  = np.zeros((M, D))\n    covars = np.zeros((M, D))\n\n    for i in range(M):\n        dot_product = np.dot(X.T, np.exp(log_gamma[:, i]))\n        means[i, :] = dot_product / np.sum(np.exp(log_gamma[:, i]))\n\n        C = X.T - means[i, :].reshape((D, 1))\n\n        res = 0\n        for j in range(N):\n            res = res + np.exp(log_gamma[j, i]) * np.outer(C[:, j], C[:, j])\n\n        covars[i, :] = np.diag(res) / np.sum(np.exp(log_gamma[:, i]))\n\n    covars[covars < varianceFloor] = varianceFloor\n\n    return means, covars\n\n\nif __name__ == \"__main__\":\n\n    data = np.load('lab2_data.npz', allow_pickle=True)['data']\n    example = np.load('lab2_example.npz', allow_pickle=True)['example'].item()\n    # trained on only one single female speaker:\n    phoneHMMs_one = np.load('lab2_models_onespkr.npz', allow_pickle=True)['phoneHMMs'].item()\n    # trained on the entire dataset:\n    phoneHMMs_all = np.load('lab2_models_all.npz', allow_pickle=True)['phoneHMMs'].item()\n\n\n    # setting up isolated pronounciations:\n    isolated = {}\n    for digit in prondict.keys():\n        isolated[digit] = ['sil'] + prondict[digit] + ['sil']\n\n    wordHMMs = {}\n    for digit in isolated.keys():\n        wordHMMs[digit] = concatHMMs(phoneHMMs_one, isolated[digit])\n\n    print(list(wordHMMs['o'].keys()))\n    print(list(wordHMMs.keys()))\n\n\n    wordHMMs_all = {}\n    for digit in isolated.keys():\n        wordHMMs_all[digit] = concatHMMs(phoneHMMs_all, isolated[digit])\n    print(list(wordHMMs_all.keys()))\n\n    #Testing log likelihood function\n    o_obsloglik = lab2_tools.log_multivariate_normal_density_diag(example['lmfcc'], wordHMMs['o']['means'],\n                                                                  wordHMMs['o']['covars'])\n\n    print(\"Testing if likelihood is correct: \")\n    np.testing.assert_almost_equal(o_obsloglik, example['obsloglik'], 6)\n    print(\"Likelihood is correct.\")\n\n    # plotting likelihood functions\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Computed \\\"o\\\" obsloglik\")\n    axs[0].pcolormesh(o_obsloglik.T)\n    axs[1].set_title(\"Example \\\"o\\\" obsloglik\")\n    axs[1].pcolormesh(example['obsloglik'].T)\n    plt.show()\n    # The dark bars in the middle refer to 'ow', while the higher prob light bars\n    # refer the 'sil' on either side of the dark.\n\n    # Testing Forward function\n    forward_probability = forward(o_obsloglik,\n            np.log(wordHMMs['o'][\"startprob\"]),\n            np.log(wordHMMs['o'][\"transmat\"]))\n\n    print(\"Testing if forward probability is ≃ to example: \")\n    np.testing.assert_almost_equal(forward_probability, example['logalpha'], 6)\n    print(\"Likelihood is correct.\")\n\n    #  plotting forward functions:\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Computed \\\"o\\\" forward probability\")\n    axs[0].pcolormesh(forward_probability.T)\n    axs[1].set_title(\"Example \\\"o\\\" forward probability\")\n    axs[1].pcolormesh(example['logalpha'].T)\n    plt.show()\n\n    # Just looking at the differences.\n    o_obsloglik_all = lab2_tools.log_multivariate_normal_density_diag(example['lmfcc'], wordHMMs_all['o']['means'],\n                                                                  wordHMMs_all['o']['covars'])\n    \n    forward_probability_all = forward(o_obsloglik_all,\n            np.log(wordHMMs_all['o'][\"startprob\"]),\n            np.log(wordHMMs_all['o'][\"transmat\"]))\n\n    #  plotting forward functions, comparing all speakers to one:\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Computed \\\"o\\\" forward probability, from one speaker\")\n    axs[0].pcolormesh(forward_probability.T)\n    axs[1].set_title(\"Example \\\"o\\\" forward probability, from multiple\")\n    axs[1].pcolormesh(forward_probability_all.T)\n    plt.show()\n    \n\n    scores = np.zeros((44, 11))\n    for i in range(len(data)):\n        data_sample = data[i]['lmfcc']\n\n        j = 0\n        for key, HMM in wordHMMs.items():\n            log_lik = lab2_tools.log_multivariate_normal_density_diag(data_sample, HMM[\"means\"], HMM[\"covars\"])\n            forward_probability2 = forward(log_lik, np.log(HMM[\"startprob\"]), np.log(HMM[\"transmat\"]))\n            scores[i, j] = lab2_tools.logsumexp(forward_probability2[-1])\n            j += 1\n\n    scores_all = np.zeros((44, 11))\n    for i in range(len(data)):\n        data_sample = data[i]['lmfcc']\n\n        j = 0\n        for key, HMM in wordHMMs_all.items():\n            log_lik = lab2_tools.log_multivariate_normal_density_diag(data_sample, HMM[\"means\"], HMM[\"covars\"])\n            forward_probability2 = forward(log_lik, np.log(HMM[\"startprob\"]), np.log(HMM[\"transmat\"]))\n            scores_all[i, j] = lab2_tools.logsumexp(forward_probability2[-1])\n            j += 1\n\n    #  plotting forward functions, comparing all speakers to one:\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Forward scores from one speaker\")\n    axs[0].pcolormesh(scores.T)\n    axs[1].set_title(\"forward scores from multiple speakers\")\n    axs[1].pcolormesh(scores_all.T)\n    plt.show()\n\n    # Doing Maximum Likelihood\n    scores_max = np.copy(scores.T)\n    scores_max = (scores_max == scores_max.max(axis=0, keepdims=1))\n\n    scores_max_all = np.copy(scores_all.T)\n    scores_max_all = (scores_max_all == scores_max_all.max(axis=0, keepdims=1))\n\n\n    #  plotting forward functions, comparing all speakers to one:\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Forward scores Maximum Likelihood, from one speaker\")\n    axs[0].pcolormesh(scores_max)\n    axs[1].set_title(\"forward scores Maximum Likelihood, from multiple speakers\")\n    axs[1].pcolormesh(scores_max_all)\n    plt.show()\n\n    \n    # Testing Backward function\n    backward_probability = backward(o_obsloglik,\n                                  np.log(wordHMMs['o'][\"startprob\"]),\n                                  np.log(wordHMMs['o'][\"transmat\"]))\n\n    print(\"Testing if backward probability is ≃ to example: \")\n    np.testing.assert_almost_equal(backward_probability, example['logbeta'], 6)\n    print(\"Likelihood is correct.\")\n\n    # plotting forward functions:\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Computed \\\"o\\\" backward probability\")\n    axs[0].pcolormesh(backward_probability.T)\n    axs[1].set_title(\"Example \\\"o\\\" backward probability\")\n    axs[1].pcolormesh(example['logbeta'].T)\n    plt.show()\n\n\n    # Testing Viterbi function\n    viterbi_score, viterbi_path = viterbi(o_obsloglik,\n                                    np.log(wordHMMs['o'][\"startprob\"]),\n                                    np.log(wordHMMs['o'][\"transmat\"]),\n                                    False)\n\n    print(\"Testing if viterbi likelihood is ≃ to example: \")\n    np.testing.assert_almost_equal(viterbi_score, example['vloglik'], 6)\n    print(\"Likelihood is correct.\")\n    print(\"Testing if viterbi path is ≃ to example: \")\n    np.testing.assert_almost_equal(viterbi_path, example['vpath'], 6)\n    print(\"Path is correct.\")\n\n    plt.pcolormesh(forward_probability.T)\n    plt.plot(viterbi_path.T, color='black')\n    plt.title(\"Viterbi overlay\")\n    plt.show()\n\n    viterbi_scores = np.zeros((44, 11))\n    for i in range(len(data)):\n        data_sample = data[i]['lmfcc']\n        j = 0\n        for key, HMM in wordHMMs_all.items():\n            log_lik = lab2_tools.log_multivariate_normal_density_diag(data_sample, HMM[\"means\"], HMM[\"covars\"])\n            v_prob, v_path = viterbi(log_lik, np.log(HMM[\"startprob\"]), np.log(HMM[\"transmat\"]), False)\n            viterbi_scores[i, j] = v_prob\n            j += 1\n\n\n    # plotting the \"normal viterbi\"\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Computed viterbi scoring\")\n    axs[0].pcolormesh(viterbi_scores.T)\n    axs[1].set_title(\"Computed forward scoring\")\n    axs[1].pcolormesh(scores_all.T)\n    plt.show()\n\n    # plot maximum viterbi scoring\n    viterbi_scores_max = np.copy(viterbi_scores.T)\n    viterbi_scores_max = (viterbi_scores_max == viterbi_scores_max.max(axis=0, keepdims=1))\n\n    #  plotting forward functions, comparing all speakers to one:\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Viterbi scores max, from multiple speakers\")\n    axs[0].pcolormesh(viterbi_scores_max)\n    axs[1].set_title(\"forward scores Maximum Likelihood, from multiple speakers\")\n    axs[1].pcolormesh(scores_max_all)\n    plt.show()\n\n\n\n\n    # Testing State Posteriors function\n    gamma = statePosteriors(forward_probability, backward_probability)\n\n    print(\"Testing if State Posteriors is ≃ to example: \")\n    np.testing.assert_almost_equal(gamma, example['loggamma'], 6)\n    print(\"Likelihood is correct.\")\n\n    # plotting State Posteriors functions:\n    fig, axs = plt.subplots(2)\n    axs[0].set_title(\"Computed \\\"o\\\" State Posteriors\")\n    axs[0].pcolormesh(gamma.T)\n    axs[1].set_title(\"Example \\\"o\\\" State Posteriors\")\n    axs[1].pcolormesh(example['loggamma'].T)\n    plt.show()\n\n    # Testing log likeliood GMM\n    #Test gmmloglik, untested because I don't know where to get the weights\n\n    # gmmloglik(o_obsloglik, )\n\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Lab 2/lab2_proto.py	(revision 917416cf914677eae4f6d6ee85db8cce7fe1ce59)
+++ Lab 2/lab2_proto.py	(date 1588329433307)
@@ -476,12 +476,19 @@
     fig, axs = plt.subplots(2)
     axs[0].set_title("Computed \"o\" State Posteriors")
     axs[0].pcolormesh(gamma.T)
+    axs[0].plot(viterbi_path, color='black')
     axs[1].set_title("Example \"o\" State Posteriors")
     axs[1].pcolormesh(example['loggamma'].T)
     plt.show()
 
+    # Summing GMMs
+    if np.sum(np.sum(np.exp(gamma), axis=1) == 71):
+        print("State Posteriors sum to 1")
+    else:
+        print("State Posteriors don't sum to 1")
+
+
     # Testing log likeliood GMM
-    #Test gmmloglik, untested because I don't know where to get the weights
 
     # gmmloglik(o_obsloglik, )
 
